{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "UNIVERSIDAD AUTONOMA DE CHIHUAHUA\n",
        "\n",
        "Facultad de Ingeniería\n",
        "\n",
        "Friends text generation\n",
        "\n",
        "Alejandro Arturo González Flores\n",
        "\n",
        "8CC2\n",
        "\n",
        "Matricula: 348552\n",
        "\n",
        "Docente: Jesus Roberto López Santillán\n",
        "\n",
        "Data Science\n",
        "\n",
        "Chihuahua, Chih. 28 de Noviembre del 2023"
      ],
      "metadata": {
        "id": "goIWdcwtrI8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_hfyXkPN8_-z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"friends.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "lines = df['line'].tolist()"
      ],
      "metadata": {
        "id": "qxNKgGbtAYXZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(lines)\n",
        "\n",
        "print(tokenizer.texts_to_sequences([\"First\"]))\n",
        "print(tokenizer.sequences_to_texts([[25,6,10,9,4]]))\n",
        "max_id= len(tokenizer.word_index)\n",
        "print(max_id)\n",
        "\n",
        "dataset_size=sum(x for x in tokenizer.word_counts.values())\n",
        "print(type(dataset_size))\n",
        "print(dataset_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDmRxxotAcFh",
        "outputId": "9d7690c8-d804-4bd7-8a5d-b0e7ccead554"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[25, 6, 10, 9, 4]]\n",
            "['f i r s t']\n",
            "80\n",
            "<class 'int'>\n",
            "3730221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "encoded = np.array([item for sublist in sequences for item in sublist]) - 1\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VgQNdE8Afht",
        "outputId": "da5abc16-130e-4d64-a14c-cba75a97d3bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20  3  6 ...  9  1 28]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "metadata": {
        "id": "6IJ1FbaxAiAE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
        ")\n",
        "dataset = dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "QK5JLz3WAp3R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2),\n",
        "    keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])"
      ],
      "metadata": {
        "id": "pWQSiZdXA0Sy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=20)\n",
        "model.save('my_friends_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG_pikPWBRDb",
        "outputId": "097d4f2b-1d92-4a51-de38-28a7f6856265"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "104910/104910 [==============================] - 1084s 10ms/step - loss: 1.5063\n",
            "Epoch 2/20\n",
            "104910/104910 [==============================] - 1075s 10ms/step - loss: 1.4510\n",
            "Epoch 3/20\n",
            "104910/104910 [==============================] - 1058s 10ms/step - loss: 1.4395\n",
            "Epoch 4/20\n",
            "104910/104910 [==============================] - 1051s 10ms/step - loss: 1.4331\n",
            "Epoch 5/20\n",
            "104910/104910 [==============================] - 1040s 10ms/step - loss: 1.4292\n",
            "Epoch 6/20\n",
            "104910/104910 [==============================] - 1044s 10ms/step - loss: 1.4264\n",
            "Epoch 7/20\n",
            "104910/104910 [==============================] - 1047s 10ms/step - loss: 1.4243\n",
            "Epoch 8/20\n",
            "104910/104910 [==============================] - 1097s 10ms/step - loss: 1.4225\n",
            "Epoch 9/20\n",
            "104910/104910 [==============================] - 1088s 10ms/step - loss: 1.4213\n",
            "Epoch 10/20\n",
            "104910/104910 [==============================] - 1098s 10ms/step - loss: 1.4203\n",
            "Epoch 11/20\n",
            "104910/104910 [==============================] - 1097s 10ms/step - loss: 1.4192\n",
            "Epoch 12/20\n",
            "104910/104910 [==============================] - 1092s 10ms/step - loss: 1.4182\n",
            "Epoch 13/20\n",
            "104910/104910 [==============================] - 1106s 11ms/step - loss: 1.4175\n",
            "Epoch 14/20\n",
            "104910/104910 [==============================] - 1122s 11ms/step - loss: 1.4166\n",
            "Epoch 15/20\n",
            "104910/104910 [==============================] - 1073s 10ms/step - loss: 1.4162\n",
            "Epoch 16/20\n",
            "104910/104910 [==============================] - 1081s 10ms/step - loss: 1.4157\n",
            "Epoch 17/20\n",
            "104910/104910 [==============================] - 1078s 10ms/step - loss: 1.4152\n",
            "Epoch 18/20\n",
            "104910/104910 [==============================] - 1093s 10ms/step - loss: 1.4148\n",
            "Epoch 19/20\n",
            "104910/104910 [==============================] - 1083s 10ms/step - loss: 1.4144\n",
            "Epoch 20/20\n",
            "104910/104910 [==============================] - 1082s 10ms/step - loss: 1.4140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#para cuando ya tengamos el modelo cargado\n",
        "model = tf.keras.models.load_model('my_friends_model.h5')\n",
        "model.summary()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "W0nOMMmrbI8v",
        "outputId": "ab1fd955-7419-41d2-bca4-e20b0733b209"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#para cuando ya tengamos el modelo cargado\\nmodel = tf.keras.models.load_model('my_friends_model.h5')\\nmodel.summary()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(texts):\n",
        "  X = np.array(tokenizer.texts_to_sequences(texts))-1\n",
        "  return tf.one_hot(X, max_id)"
      ],
      "metadata": {
        "id": "5LGjk9g8Bg_3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = preprocess([\"How you doi\"])\n",
        "Y_pred_prob = model.predict(X_new)\n",
        "Y_pred_classes = np.argmax(Y_pred_prob, axis=-1)\n",
        "\n",
        "predicted_text = tokenizer.sequences_to_texts(Y_pred_classes + 1)[0][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryWznu75Bkzj",
        "outputId": "a0e56aad-23df-4aa9-f76e-726d0b3eb2e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 606ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature=1):\n",
        "  X_new = preprocess([text])\n",
        "  y_proba = model.predict(X_new)[0, -1:, :]\n",
        "  rescaled_logits = tf.math.log(y_proba)/temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1) +1\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "metadata": {
        "id": "yF17XFT6BnJV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text,temperature)\n",
        "  return text"
      ],
      "metadata": {
        "id": "LxGtSBy9BpRX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_text(\"t\", temperature=0.2))\n",
        "print(complete_text(\"w\", temperature=1))\n",
        "print(complete_text(\"w\", temperature=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3M1Q5veBrVk",
        "outputId": "7697a4c8-a6e2-4fdd-acc9-5fdfb8e0cee7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 559ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "the mets on the restles in the hole) oh, you know w\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "we gotta ofe me it! wow yourse right? you know... t\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "who'? ayicuadly bon! pugsec’ i' did primabe usud eh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "batch_size = 32\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
        "datasets = []\n",
        "for encoded_part in encoded_parts:\n",
        "  dataset= tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "  dataset= dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "  dataset= dataset.flat_map(lambda window: window.batch(window_length))\n",
        "  datasets.append(dataset)\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
        ")\n",
        "dataset= dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "SH4_K0maBx83"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stateful_model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, batch_input_shape=[batch_size, None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])"
      ],
      "metadata": {
        "id": "pDgaHTiLBznU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_begin(self, epoch, logs):\n",
        "    self.model.reset_states()"
      ],
      "metadata": {
        "id": "RStEqaWxB3qA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "model.fit(dataset,epochs=50, callbacks=[ResetStatesCallback()])\n",
        "model.save('my_stateful_friends_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECHmjsxOB5-K",
        "outputId": "60d07735-06f1-4ee1-e6f0-cbaebb2b7f47"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1049/1049 [==============================] - 14s 10ms/step - loss: 1.6526\n",
            "Epoch 2/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.6080\n",
            "Epoch 3/50\n",
            "1049/1049 [==============================] - 11s 11ms/step - loss: 1.5987\n",
            "Epoch 4/50\n",
            "1049/1049 [==============================] - 12s 11ms/step - loss: 1.5938\n",
            "Epoch 5/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5901\n",
            "Epoch 6/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5879\n",
            "Epoch 7/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5845\n",
            "Epoch 8/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5837\n",
            "Epoch 9/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5844\n",
            "Epoch 10/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5830\n",
            "Epoch 11/50\n",
            "1049/1049 [==============================] - 11s 11ms/step - loss: 1.5813\n",
            "Epoch 12/50\n",
            "1049/1049 [==============================] - 11s 11ms/step - loss: 1.5805\n",
            "Epoch 13/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5793\n",
            "Epoch 14/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5797\n",
            "Epoch 15/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5790\n",
            "Epoch 16/50\n",
            "1049/1049 [==============================] - 11s 11ms/step - loss: 1.5788\n",
            "Epoch 17/50\n",
            "1049/1049 [==============================] - 11s 11ms/step - loss: 1.5787\n",
            "Epoch 18/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5777\n",
            "Epoch 19/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5781\n",
            "Epoch 20/50\n",
            "1049/1049 [==============================] - 11s 11ms/step - loss: 1.5773\n",
            "Epoch 21/50\n",
            "1049/1049 [==============================] - 11s 11ms/step - loss: 1.5778\n",
            "Epoch 22/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5755\n",
            "Epoch 23/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5761\n",
            "Epoch 24/50\n",
            "1049/1049 [==============================] - 11s 11ms/step - loss: 1.5764\n",
            "Epoch 25/50\n",
            "1049/1049 [==============================] - 11s 11ms/step - loss: 1.5765\n",
            "Epoch 26/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5760\n",
            "Epoch 27/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5761\n",
            "Epoch 28/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5741\n",
            "Epoch 29/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5752\n",
            "Epoch 30/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5745\n",
            "Epoch 31/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5749\n",
            "Epoch 32/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5747\n",
            "Epoch 33/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5749\n",
            "Epoch 34/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5739\n",
            "Epoch 35/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5737\n",
            "Epoch 36/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5741\n",
            "Epoch 37/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5733\n",
            "Epoch 38/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5738\n",
            "Epoch 39/50\n",
            "1049/1049 [==============================] - 11s 11ms/step - loss: 1.5722\n",
            "Epoch 40/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5747\n",
            "Epoch 41/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5735\n",
            "Epoch 42/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5742\n",
            "Epoch 43/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5730\n",
            "Epoch 44/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5725\n",
            "Epoch 45/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5733\n",
            "Epoch 46/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5743\n",
            "Epoch 47/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5738\n",
            "Epoch 48/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5730\n",
            "Epoch 49/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5724\n",
            "Epoch 50/50\n",
            "1049/1049 [==============================] - 11s 10ms/step - loss: 1.5733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stateless_model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])"
      ],
      "metadata": {
        "id": "INK1gqjvbaxW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stateless_model.build(tf.TensorShape([None,None,max_id]))\n",
        "stateless_model.set_weights(model.get_weights())\n",
        "stateless_model.save('my_stateless_friends_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x6QVRgDbcZ_",
        "outputId": "294b8455-6137-4f69-cd88-e0905d3c5240"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "print(complete_text(\"t\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duetnzp2beN9",
        "outputId": "127abd44-e70e-4e92-eb30-e685f6084450"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 555ms/step\n",
            "1/1 [==============================] - 1s 559ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "this time when it is a rox more hour, and your own \n"
          ]
        }
      ]
    }
  ]
}